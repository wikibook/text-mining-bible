{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Korean_movie_reviews_2016.txt', encoding='utf-8') as f:\n",
    "    docs = [doc.strip().split('\\t') for doc in f]\n",
    "    docs = [(doc[0], int(doc[1])) for doc in docs if len(doc) == 2]\n",
    "    texts, labels = zip(*docs) # 둘을 분리해서 별도의 list 변수로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To split the data into training and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CounterVectorizer 클래스를 이용한 벡터 표현\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vectorizer = CountVectorizer() \n",
    "tf_train_features = tf_vectorizer.fit_transform(train_texts) \n",
    "tf_test_features = tf_vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = GradientBoostingClassifier(max_depth=2, n_estimators=300, random_state=2, learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=1.0, max_depth=2, n_estimators=300,\n",
       "                           random_state=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_clf.fit(tf_train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels_tf = gb_clf.predict(tf_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.83      7766\n",
      "           1       0.83      0.90      0.86      8773\n",
      "\n",
      "    accuracy                           0.85     16539\n",
      "   macro avg       0.85      0.84      0.85     16539\n",
      "weighted avg       0.85      0.85      0.85     16539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, pred_labels_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.846026773404942"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(test_labels, pred_labels_tf, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators 값 변경해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimators: 10 , Score: 0.6784392545754462\n",
      "Number of estimators: 50 , Score: 0.7820051717989631\n",
      "Number of estimators: 100 , Score: 0.811061913512709\n",
      "Number of estimators: 300 , Score: 0.846026773404942\n",
      "Number of estimators: 500 , Score: 0.8589618955476586\n",
      "Number of estimators: 700 , Score: 0.8657011556835132\n",
      "Number of estimators: 1000 , Score: 0.8688241194847548\n"
     ]
    }
   ],
   "source": [
    "n_estimators_values = [10, 50, 100, 300, 500, 700, 1000]\n",
    "for value in n_estimators_values:\n",
    "    gbr = GradientBoostingClassifier(max_depth=2, n_estimators=value, random_state=2, learning_rate=1.0)\n",
    "    gbr.fit(tf_train_features, train_labels)\n",
    "    y_preds = gbr.predict(tf_test_features)\n",
    "    f1 = f1_score(test_labels, y_preds, average='macro')\n",
    "    print('Number of estimators:', value, ', Score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.01 , Score: 0.6906814272563049\n",
      "Learning rate: 0.1 , Score: 0.8010863578544098\n",
      "Learning rate: 0.3 , Score: 0.8390737037497261\n",
      "Learning rate: 0.5 , Score: 0.853282905191383\n",
      "Learning rate: 1.0 , Score: 0.8589618955476586\n"
     ]
    }
   ],
   "source": [
    "learning_rate_values = [0.01, 0.1, 0.3, 0.5, 1.0]\n",
    "for value in learning_rate_values:\n",
    "    gbr = GradientBoostingClassifier(max_depth=2, n_estimators=500, random_state=2, learning_rate=value)\n",
    "    gbr.fit(tf_train_features, train_labels)\n",
    "    y_preds = gbr.predict(tf_test_features)\n",
    "    f1 = f1_score(test_labels, y_preds, average='macro')\n",
    "    print('Learning rate:', value, ', Score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.1 , Score: 0.8190527603070623\n",
      "Learning rate: 0.3 , Score: 0.8423571326394537\n",
      "Learning rate: 0.5 , Score: 0.8550180840632975\n",
      "Learning rate: 1 , Score: 0.8589618955476586\n"
     ]
    }
   ],
   "source": [
    "subsample_values = [0.1, 0.3, 0.5, 1]\n",
    "for value in subsample_values:\n",
    "    gbr = GradientBoostingClassifier(max_depth=2, n_estimators=500, random_state=2, \n",
    "                                     learning_rate=1.0, subsample=value)\n",
    "    gbr.fit(tf_train_features, train_labels)\n",
    "    y_preds = gbr.predict(tf_test_features)\n",
    "    f1 = f1_score(test_labels, y_preds, average='macro')\n",
    "    print('Subsample_ratio:', value, ', Score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
